 # Scrape inside individual search results
    for i in range(len(link_results)):

        soup = create_soup_from_url(url=link_results[i])

         # Get article body and individual paragraphs within
        article_body = soup.find('div', attrs={'class':'article__body'}).findAll('p', recursive=False)

        list_paragraphs = []
        for paragraph in article_body:

            list_paragraphs.append(paragraph.text)
            complete_article = " ".join(list_paragraphs)

        article_results.append(complete_article)

        # Get article title
        try:
            article_title = soup.find('section', attrs={'class':'article__main'}).find('h1', recursive=False)
            title_results.append(article_title.text)
        except:
            article_title = 'N/A'
            title_results.append(article_title)

    # Add articles and titles to pandas dataframe
    articles_list = {'Article': article_results, 'Title': title_results, 'Date Scraped': datetime.now()}
    articles_df = pd.DataFrame(data=articles_list)
    cols = ['Article', 'Title', 'Date Scraped']
    articles_df = articles_df[cols]
    # Check for duplicates in df:
    # articles_df = articles_df[articles_df.duplicated()]
    # Drop duplicates in df:
    articles_df = articles_df.drop_duplicates()
    articles_df.to_csv(r'data/test_efsyn_articles.txt', index=False, sep=' ', header=False)


    # A couple of development sanity checks:
    print(link_results)
    print(len(link_results))

    # Check if link_results are unique:
    if len(link_results) > len(set(link_results)):
        print("not unique")
    else:
        print("unique")

    print(articles_df)